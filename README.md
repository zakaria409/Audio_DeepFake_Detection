# Audio Authentication for Banking Systems

## Description

In an era where artificial intelligence has advanced to the point of creating highly realistic deepfake audio, the need for robust detection systems has never been more critical. This project focuses on developing an advanced audio authentication system tailored for banking environments. The system aims to detect and identify audio spoofing attacks, ensuring the integrity and security of voice-driven transactions. By leveraging sophisticated machine learning models and comprehensive datasets, this project addresses the growing threat of audio deepfakes, particularly in financial fraud scenarios documented by the Federal Trade Commission and public incidents like the USA election.

## Features

- **Dataset Preparation**: Compiled a diverse dataset from ASVspoof 2019 LA, DEEP-VOICE, and WaveFake datasets. Analyzed and refined these datasets to create a robust benchmark for evaluating detection algorithms.
- **Model Evaluation**: Utilized Mel-Frequency Cepstral Coefficients (MFCCs) and Mel-spectrograms for feature extraction. Evaluated models including Long Short-Term Memory (LSTM), ResNet, U-Net, and others.
- **GUI Functionality**: Developed a simple GUI using Tkinter, allowing users to input audio files or record audio for real-time analysis. The GUI displays results from both the spoofing detection and identification models.
- **Results Display**: Presented performance metrics for the refined dataset compared to the ASVspoof dataset, highlighting improvements and insights.

## Dataset

### Datasets Used

- **ASVspoof 2019 LA**: Focuses on detecting synthetic speech, including text-to-speech (TTS) and voice conversion (VC) attacks.
- **DEEP-VOICE**: Curated to tackle the challenges of detecting deepfake voices, providing high-quality, realistic voice synthesis samples.
- **WaveFake**: Designed to evaluate the robustness of spoofing detection systems against audio deepfakes, covering a diverse range of synthetic speech generated by different deep learning models.

### Dataset Refinement

The datasets were analyzed for variety in transmission, encoding, and deep fake generating methods. A refined dataset was created to ensure diversity and comprehensiveness, providing a solid foundation for evaluating and comparing detection algorithms.

## Models and Methodologies

### Feature Extraction

- **Mel-Frequency Cepstral Coefficients (MFCCs)**: Used for capturing the spectral properties of speech.
- **Mel-spectrograms**: Fused with MFCCs using Principal Component Analysis (PCA) for enhanced feature representation.

### Model Evaluation

- **LSTM**: Effective for sequence prediction but struggled with data complexity.
- **ResNet**: High performance but exhibited high variance.
- **U-Net**: Chosen for its balanced performance and stability across various considerations.
- **VGGish**: Scored the best accuracy but was too large for practical use.
- **Other Models**: XceptionNet and GRU showed better performance on the refined dataset but indicated overfitting.

## GUI

The GUI, built with Tkinter, allows users to:
- Input audio files of any type.
- Record audio for real-time analysis.
- Pass the audio to both the spoofing detection and identification models.
- Display results for user verification.

## Results

### Performance Metrics

| Model       | ASVspoof 2019 LA | Refined Dataset | Refined/ASVspoof |
|-------------|------------------|-----------------|------------------|
| LSTM        | 74.68            | 92.67           | 89.70            |
| ResNet      | 91.63            | 93.64           | 94.78            |
| M-Transformer| 91.38           | 92.29           | 95.63            |
| WaveNet     | 86.24            | 92.99           | 94.01            |
| U-Net       | 82.39            | 93.50           | 95.17            |
| XceptionNet | 82.39            | 93.71           | 91.65            |
| VGGish      | 93.81            | 96.27           | 97.86            |
| Wave-U-Net  | 67.36            | 94.63           | 95.23            |
| GRU         | 92.49            | 80.63           | 78.10            |
| Wave2Vec    | 94.99            | 91.21           | 93.90            |

## Installation

### Dependencies

- Python 3.3.3
- TensorFlow 2.15.0
- Scikit-learn
- Librosa
- Tkinter

### Setup Instructions

Versions are specified at the end of the development notebook

## Usage

1. Launch the application using the instructions provided in the Installation section.
2. Use the GUI to input an audio file or record audio.
3. The application will process the audio through both the spoofing detection and identification models.
4. View the results displayed on the GUI.

## Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository.
2. Create a new branch (`git checkout -b feature-branch`).
3. Make your changes and commit them (`git commit -m 'Add some feature'`).
4. Push to the branch (`git push origin feature-branch`).
5. Open a pull request.

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

## Acknowledgments

- **Datasets**: ASVspoof 2019 LA, DEEP-VOICE, and WaveFake datasets from Kaggle.
- **Libraries**: TensorFlow, Scikit-learn, Librosa, and Tkinter.
